{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pong_Mine_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkPaM+ScXYUyaLbEbEK9IY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazem9806/Deep_Learning/blob/Artificial_Neural_Network/Pong_Mine_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OBf1wqFT4K9"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "from torch.distributions import Categorical\n",
        "from itertools import count\n",
        "\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6pjNDzgBzho"
      },
      "source": [
        "'''\n",
        "    Pong require a user to press the FIRE button to start the game.\n",
        "    The following code corresponds to the wrapper FireResetEnvthat presses the FIRE button in \n",
        "    environments that require that for the game to start.\n",
        "    \n",
        "    In addition to pressing FIRE, this wrapper checks for several corner cases that are present in some games.\n",
        "\n",
        "'''\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12kyEB0tBzbw"
      },
      "source": [
        "'''\n",
        "\n",
        "The next wrapper that we will require is MaxAndSkipEnv that codes a couple of important transformations for Pong\n",
        "'''\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "    def step(self, action):\n",
        "            total_reward = 0.0\n",
        "            done = None\n",
        "            for _ in range(self._skip):\n",
        "              obs, reward, done, info = self.env.step(action)\n",
        "              self._obs_buffer.append(obs)\n",
        "              total_reward += reward\n",
        "              if done:\n",
        "                  break\n",
        "            max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "            return max_frame, total_reward, done, info\n",
        "    def reset(self):\n",
        "          self._obs_buffer.clear()\n",
        "          obs = self.env.reset()\n",
        "          self._obs_buffer.append(obs)\n",
        "          return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_SyUCCBzWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPncg9c_EhnK"
      },
      "source": [
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFaKarcSBzQw"
      },
      "source": [
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliZiBeDBzJF"
      },
      "source": [
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "      ## moveaxis --> Move axes of an array to new positions\n",
        "      ## as the Conv2d takes the image argument as (channels,height, width)\n",
        "        return np.moveaxis(observation, 2, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ0IwPlzCWnp"
      },
      "source": [
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0qqniOcT4bc"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 128, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "#        print(',-- torch.zeros(1, *shape): ',torch.zeros(1, *shape), ', -- shape: ', shape, ', \\nreturn data: ',int(np.prod(o.size())))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return Categorical(torch.softmax(self.policy(conv_out), dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfIr7jS0dsS0"
      },
      "source": [
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 128, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.critic(conv_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqOtC4ZqpfB0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT8LeU8qh2PX"
      },
      "source": [
        "'''\n",
        "    Before feeding the frames to the neural network every frame is scaled down from 210x160, \n",
        "    with three color frames (RGB color channels), \n",
        "    to a single-color 84 x84 image using a colorimetric grayscale conversion. \n",
        "    Different approaches are possible. \n",
        "    One of them is cropping non-relevant parts of the image and then scaling down\n",
        "'''\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Downsamples image to 84x84\n",
        "    Greyscales image\n",
        "    Returns numpy array\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "#        print('frame size: ',frame.size)\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "#        print('RGB image size: ', img.size)\n",
        "\n",
        "        ## Conversion from RGB to Gray Scale --> b as [0.2989, 0.5870, 0.1140]\n",
        "        Gray_Scale_Parameters = [0.2989, 0.5870, 0.1140]\n",
        "        img = img[:, :, 0] * Gray_Scale_Parameters[0] + img[:, :, 1] * Gray_Scale_Parameters[1] + img[:, :, 2] * Gray_Scale_Parameters[2]\n",
        "        thresh = 120\n",
        "        img = cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY)[1]\n",
        "#        cv2.imwrite('/content/sample_data/binary_image.png', img)\n",
        "#        print('Gray Scale image size: ', img.shape)\n",
        "        img = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        img = img[18:102, :]\n",
        "        img = np.reshape(img, [84, 84, 1])\n",
        "#        print(img.dtype)\n",
        "        #cv2.imwrite('/content/sample_data/img.png', img)\n",
        "#        print(img.shape)\n",
        "        #img2 = img.astype(np.uint8)\n",
        "        #img2 = cv2.threshold(img2,127,255,cv2.THRESH_BINARY)\n",
        "#        cv2_imshow(img)\n",
        "        #cv2.imwrite(img2,'/content/sample_data/img.png')\n",
        "        return img.astype(np.uint8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqUzyVNuiCPN"
      },
      "source": [
        "# env = make_env(ENVIROMENT_NAME)\n",
        "# while True:\n",
        "#   state = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhH7oeILM3nY"
      },
      "source": [
        "## function that takes a list of rewards and reutrn the list of returns for each step\n",
        "def learn_step(action_log_prob, critic, value, reward, done, next_state, optimizerA, optimizerC, gamma=0.9):\n",
        "    target_q = reward + gamma * (1 - int(done)) * critic(torch.FloatTensor([next_state]))\n",
        "    advantage = target_q - value\n",
        "\n",
        "    ## compute losses\n",
        "    optimizerA.zero_grad()\n",
        "    actor_loss = -action_log_prob * advantage.detach()\n",
        "    actor_loss.backward()\n",
        "    optimizerA.step()\n",
        "\n",
        "    optimizerC.zero_grad()\n",
        "    critic_loss = advantage.pow(2)\n",
        "    critic_loss.backward()\n",
        "    optimizerC.step()\n",
        "        ## Init R\n",
        "    #R = 0\n",
        "    #returns = list()\n",
        "    #for reward in reversed(rewards):\n",
        "    #    R = reward + gamma * R\n",
        "    #    #print(R)\n",
        "    #    returns.insert(0, R)\n",
        "    #    #returns.append(R)\n",
        "\n",
        "    #returns = torch.tensor(returns)\n",
        "    ## normalize the returns\n",
        "    #returns = (returns - returns.mean()) / (returns.std() + 1e-6)\n",
        "    #return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xszx0PyC_Prq"
      },
      "source": [
        "# env = make_env(ENVIROMENT_NAME)\n",
        "\n",
        "\n",
        "# #observations = env.reset()\n",
        "\n",
        "# GPU = False\n",
        "# device = (\"cuda\" if GPU else \"cpu\")\n",
        "# #print('env.observation_space.shape: ',env.observation_space.shape)\n",
        "# policy_nn = PolicyNet(env.observation_space.shape, env.action_space.n).to(device)\n",
        "# critic_nn = CriticNet(env.observation_space.shape).to(device)\n",
        "# #print(policy_nn)\n",
        "# #print(critic_nn)\n",
        "\n",
        "# ## initialize an optimizer\n",
        "# policy_optimizer = torch.optim.Adam(policy_nn.parameters(), lr=1e-3)\n",
        "# critic_optimizer = torch.optim.Adam(critic_nn.parameters(), lr=1e-3)\n",
        "\n",
        "# state = env.reset()\n",
        "# #print(state.shape[0])\n",
        "\n",
        "\n",
        "# action_log_probs = list()\n",
        "# rewards = list()\n",
        "# values = list()\n",
        "# next_states = list()\n",
        "# running_reward = 10\n",
        "# #while True:\n",
        "# ## take an action sampled from a categorical distribution given the state\n",
        "# action_prob = policy_nn(torch.FloatTensor(state).unsqueeze(0))\n",
        "# action = action_prob.sample()\n",
        "# action_log_prob = action_prob.log_prob(action)\n",
        "\n",
        "# #print(action_log_prob)\n",
        "# # print(action)\n",
        "# # print(action_log_prob)\n",
        "\n",
        "# value = critic_nn(torch.FloatTensor(state).unsqueeze(0))\n",
        "# #print(value)\n",
        "# #values.append(value[0])\n",
        "# #print(action)\n",
        "\n",
        "# next_state, reward, is_done, _ = env.step(action.item()) # take a random action\n",
        "# rewards.append(reward)\n",
        "\n",
        "# ## do a learning step (online) ##\n",
        "# learn_step(action_log_prob, critic_nn, value, reward, is_done, next_state, policy_optimizer, critic_optimizer)\n",
        "\n",
        "# ## current state is next state now\n",
        "# state = next_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjJNvE2gstV"
      },
      "source": [
        "ENVIROMENT_NAME = 'Pong-v0'\n",
        "\n",
        "##################################################################\n",
        "##                   HYPERPARAMETERS                            ##\n",
        "##################################################################\n",
        "GAMMA = 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz8yvKKeT4gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13123345-6414-4fda-b547-345029e83c24"
      },
      "source": [
        "env = make_env(ENVIROMENT_NAME)\n",
        "\n",
        "\n",
        "#observations = env.reset()\n",
        "\n",
        "GPU = False\n",
        "device = (\"cuda\" if GPU else \"cpu\")\n",
        "print('env.observation_space.shape: ',env.observation_space.shape)\n",
        "policy_nn = PolicyNet(env.observation_space.shape, env.action_space.n).to(device)\n",
        "critic_nn = CriticNet(env.observation_space.shape).to(device)\n",
        "#print(policy_nn)\n",
        "#print(critic_nn)\n",
        "\n",
        "## initialize an optimizer\n",
        "policy_optimizer = torch.optim.Adam(policy_nn.parameters(), lr=1e-3)\n",
        "critic_optimizer = torch.optim.Adam(critic_nn.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "#print(state.shape[0])\n",
        "running_reward = 10\n",
        "for e in count(1):\n",
        "\n",
        "  action_log_probs = list()\n",
        "  rewards = list()\n",
        "  values = list()\n",
        "  next_states = list()\n",
        "  state = env.reset()\n",
        "\n",
        "  for t in range(100000):\n",
        "    ## take an action sampled from a categorical distribution given the state\n",
        "    action_prob = policy_nn(torch.FloatTensor(state).unsqueeze(0))\n",
        "    action = action_prob.sample()\n",
        "    action_log_prob = action_prob.log_prob(action)\n",
        "\n",
        "    # print(action_prob)\n",
        "    # print(action)\n",
        "    # print(action_log_prob)\n",
        "\n",
        "    value = critic_nn(torch.FloatTensor(state).unsqueeze(0))\n",
        "    #values.append(value[0])\n",
        "    #print(action)\n",
        "\n",
        "    next_state, reward, is_done, _ = env.step(action.item()) # take a random action\n",
        "    rewards.append(reward)\n",
        "\n",
        "    ## do a learning step (online) ##\n",
        "    #learn_step(action_log_prob, critic_nn, value, reward, is_done, next_state, policy_optimizer, critic_optimizer)\n",
        "    #def learn_step(action_log_prob, critic, value, reward, done, next_state, optimizerA, optimizerC, gamma=0.9):\n",
        "\n",
        "    target_q = reward + GAMMA * (1 - int(is_done)) * critic_nn(torch.FloatTensor([next_state]))\n",
        "    advantage = target_q - value\n",
        "\n",
        "    ## compute losses\n",
        "    policy_optimizer.zero_grad()\n",
        "    actor_loss = -action_log_prob * advantage.detach()\n",
        "    actor_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss = advantage.pow(2)\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()\n",
        "    \n",
        "    ## current state is next state now\n",
        "    state = next_state\n",
        "\n",
        "    if is_done:\n",
        "      #print(rewards)\n",
        "      #print(values)\n",
        "      break\n",
        "\n",
        "    ## get stats\n",
        "  ep_reward = sum(rewards)\n",
        "  running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "  if e % 10 == 0:\n",
        "      print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                e, ep_reward, running_reward))\n",
        "  if (env.spec.reward_threshold is not None):\n",
        "    if (running_reward > env.spec.reward_threshold):\n",
        "        print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.observation_space.shape:  (4, 84, 84)\n",
            "Episode 10\tLast reward: -21.00\tAverage reward: -2.44\n",
            "Episode 20\tLast reward: -21.00\tAverage reward: -9.89\n",
            "Episode 30\tLast reward: -21.00\tAverage reward: -14.35\n",
            "Episode 40\tLast reward: -21.00\tAverage reward: -17.02\n",
            "Episode 50\tLast reward: -21.00\tAverage reward: -18.61\n",
            "Episode 60\tLast reward: -21.00\tAverage reward: -19.57\n",
            "Episode 70\tLast reward: -21.00\tAverage reward: -20.14\n",
            "Episode 80\tLast reward: -21.00\tAverage reward: -20.49\n",
            "Episode 90\tLast reward: -21.00\tAverage reward: -20.69\n",
            "Episode 100\tLast reward: -21.00\tAverage reward: -20.82\n",
            "Episode 110\tLast reward: -21.00\tAverage reward: -20.89\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}